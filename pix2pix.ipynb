{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bkkaggle/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wNjDKdQy35h"
   },
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TRm-USlsHgEV"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pt3igws3eiVp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('pytorch-CycleGAN-and-pix2pix/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z1EySlOXwwoa"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8daqlgVhw29P"
   },
   "source": [
    "# Datasets\n",
    "\n",
    "Download one of the official datasets with:\n",
    "\n",
    "-   `bash ./datasets/download_pix2pix_dataset.sh [cityscapes, night2day, edges2handbags, edges2shoes, facades, maps]`\n",
    "\n",
    "Or use your own dataset by creating the appropriate folders and adding in the images. Follow the instructions [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md#pix2pix-datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrdOettJxaCc"
   },
   "outputs": [],
   "source": [
    "!bash ./datasets/download_pix2pix_dataset.sh facades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdUz4116xhpm"
   },
   "source": [
    "# Pretrained models\n",
    "\n",
    "Download one of the official pretrained models with:\n",
    "\n",
    "-   `bash ./scripts/download_pix2pix_model.sh [edges2shoes, sat2map, map2sat, facades_label2photo, and day2night]`\n",
    "\n",
    "Or add your own pretrained model to `./checkpoints/{NAME}_pretrained/latest_net_G.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GC2DEP4M0OsS"
   },
   "outputs": [],
   "source": [
    "!bash ./scripts/download_pix2pix_model.sh facades_label2photo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFw1kDQBx3LN"
   },
   "source": [
    "# Training\n",
    "\n",
    "-   `python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA`\n",
    "\n",
    "Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size. Add `--direction BtoA` if you want to train a model to transfrom from class B to A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0sp7TCT2x9dB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./datasets/shapes/final       \t[default: None]\n",
      "             dataset_mode: aligned                       \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: -1                            \t[default: 1]\n",
      "            display_ncols: 4                             \n",
      "             display_port: 8097                          \n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: vanilla                       \n",
      "                  gpu_ids: -1                            \t[default: 0]\n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                lambda_L1: 100.0                         \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 286                           \n",
      "                       lr: 0.0002                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: inf                           \n",
      "                    model: pix2pix                       \t[default: cycle_gan]\n",
      "                 n_epochs: 5                             \t[default: 100]\n",
      "           n_epochs_decay: 5                             \t[default: 100]\n",
      "               n_layers_D: 3                             \n",
      "                     name: shapes_pix2pix                \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: unet_256                      \n",
      "                      ngf: 64                            \n",
      "               no_dropout: False                         \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: batch                         \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 0                             \n",
      "               preprocess: resize_and_crop               \n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 5                             \n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n",
      "dataset [AlignedDataset] was created\n",
      "The number of training images = 20\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [Pix2PixModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 54.414 M\n",
      "[Network D] Total number of parameters : 2.769 M\n",
      "-----------------------------------------------\n",
      "create web directory ./checkpoints/shapes_pix2pix/web...\n",
      "/opt/homebrew/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 1 / 10 \t Time Taken: 31 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 2 / 10 \t Time Taken: 31 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 3 / 10 \t Time Taken: 31 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 4 / 10 \t Time Taken: 30 sec\n",
      "learning rate 0.0002000 -> 0.0001667\n",
      "(epoch: 5, iters: 20, time: 0.429, data: 1.956) G_GAN: 1.614 G_L1: 30.558 D_real: 0.481 D_fake: 0.275 \n",
      "saving the model at the end of epoch 5, iters 100\n",
      "End of epoch 5 / 10 \t Time Taken: 32 sec\n",
      "learning rate 0.0001667 -> 0.0001333\n",
      "End of epoch 6 / 10 \t Time Taken: 30 sec\n",
      "learning rate 0.0001333 -> 0.0001000\n",
      "End of epoch 7 / 10 \t Time Taken: 30 sec\n",
      "learning rate 0.0001000 -> 0.0000667\n",
      "End of epoch 8 / 10 \t Time Taken: 31 sec\n",
      "learning rate 0.0000667 -> 0.0000333\n",
      "End of epoch 9 / 10 \t Time Taken: 31 sec\n",
      "learning rate 0.0000333 -> 0.0000000\n",
      "(epoch: 10, iters: 20, time: 0.412, data: 2.264) G_GAN: 0.717 G_L1: 4.281 D_real: 0.126 D_fake: 0.747 \n",
      "saving the model at the end of epoch 10, iters 200\n",
      "End of epoch 10 / 10 \t Time Taken: 32 sec\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot ./datasets/shapes/final --name shapes_pix2pix --model pix2pix --direction AtoB --display_id -1 --gpu_ids -1 --n_epochs 5 --n_epochs_decay 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UkcaFZiyASl"
   },
   "source": [
    "# Testing\n",
    "\n",
    "-   `python test.py --dataroot ./datasets/facades --direction BtoA --model pix2pix --name facades_pix2pix`\n",
    "\n",
    "Change the `--dataroot`, `--name`, and `--direction` to be consistent with your trained model's configuration and how you want to transform images.\n",
    "\n",
    "> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:\n",
    "> Note that we specified --direction BtoA as Facades dataset's A to B direction is photos to labels.\n",
    "\n",
    "> If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use --model test option. See ./scripts/test_single.sh for how to apply a model to Facade label maps (stored in the directory facades/testB).\n",
    "\n",
    "> See a list of currently available models at ./scripts/download_pix2pix_model.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mey7o6j-0368"
   },
   "outputs": [],
   "source": [
    "!ls checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCsKkEq0yGh0"
   },
   "outputs": [],
   "source": [
    "!python test.py --dataroot ./datasets/shapes/final --direction AtoB --model pix2pix --name shapes_pix2pix --use_wandb --gpu_ids -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OzSKIPUByfiN"
   },
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Mgg8raPyizq"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = plt.imread('./results/shapes_pix2pix/test_latest/images/1_fake_B.png')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0G3oVH9DyqLQ"
   },
   "outputs": [],
   "source": [
    "img = plt.imread('./results/shapes_pix2pix/test_latest/images/1_real_A.png')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErK5OC1j1LH4"
   },
   "outputs": [],
   "source": [
    "img = plt.imread('./results/shapes_pix2pix/test_latest/images/1_real_B.png')\n",
    "plt.imshow(img)\n",
    "img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "UnetGenerator.__init__() missing 3 required positional arguments: 'input_nc', 'output_nc', and 'num_downs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmodels\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnetworks\u001b[39;00m \u001b[39mimport\u001b[39;00m UnetGenerator\n\u001b[0;32m---> 10\u001b[0m model \u001b[39m=\u001b[39m UnetGenerator()\n\u001b[1;32m     11\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mcheckpoints/shapes_pix2pix/latest_net_G.pth\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     13\u001b[0m \u001b[39m# opt = TestOptions().parse(args_string='--dataroot ./datasets/shapes/final --direction AtoB --model pix2pix --name shapes_pix2pix --use_wandb --gpu_ids -1')\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# hard-code some parameters for test\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# opt.num_threads = 0   # test code only supports num_threads = 0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m# plt.imshow(visuals)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: UnetGenerator.__init__() missing 3 required positional arguments: 'input_nc', 'output_nc', and 'num_downs'"
     ]
    }
   ],
   "source": [
    "from options.test_options import TestOptions\n",
    "from data import create_dataset\n",
    "from models import create_model\n",
    "\n",
    "import torch\n",
    "import models\n",
    "\n",
    "from models.networks import UnetGenerator\n",
    "\n",
    "model = UnetGenerator()\n",
    "model.load_state_dict(torch.load('checkpoints/shapes_pix2pix/latest_net_G.pth'))\n",
    "\n",
    "# opt = TestOptions().parse(args_string='--dataroot ./datasets/shapes/final --direction AtoB --model pix2pix --name shapes_pix2pix --use_wandb --gpu_ids -1')\n",
    "# hard-code some parameters for test\n",
    "# opt.num_threads = 0   # test code only supports num_threads = 0\n",
    "# opt.batch_size = 1    # test code only supports batch_size = 1\n",
    "# opt.serial_batches = True  # disable data shuffling; comment this line if results on randomly chosen images are needed.\n",
    "# opt.no_flip = True    # no flip; comment this line if results on flipped images are needed.\n",
    "# opt.display_id = -1   # no visdom display; the test code saves the results to a HTML file.\n",
    "# dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options\n",
    "# model = create_model(opt)      # create a model given opt.model and other options\n",
    "# model.setup(opt)               # regular setup:\n",
    "\n",
    "# load the generator model\n",
    "# model = models.find_model_using_name('pix2pix')\n",
    "# model = torch.load('checkpoints/shapes_pix2pix/latest_net_G.pth')\n",
    "# model.eval()\n",
    "\n",
    "# img = plt.imread('datasets/shapes/A/test/1.png')\n",
    "\n",
    "\n",
    "# model.set_input(img)  # unpack data from data loader\n",
    "# model.test()           # run inference\n",
    "# visuals = model.get_current_visuals()  # get image results\n",
    "# img_path = model.get_image_paths()    \n",
    "\n",
    "# plt.imshow(img)\n",
    "# img\n",
    "\n",
    "\n",
    "# plt.imshow(visuals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1394e26d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img = plt.imread('datasets/shapes/final/train/1.png')\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "pix2pix",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
